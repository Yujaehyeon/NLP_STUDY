{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# to make a simple list out of list of lists \n",
    "flatten = lambda l: [item for sublist in l for item in sublist] \n",
    "# which means:  for sublist in I: \n",
    "#                  for item in sublist:\n",
    "#                      flatten.append(item) \n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "# gpus = [0]\n",
    "# torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        # eindex -> sindex 로 \n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "        \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# four types of named entities: \n",
    "# persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.\n",
    "corpus = nltk.corpus.conll2002.iob_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The first item on each line is a word and the second the named entity tag.\n",
    "# B denotes the first item of a phrase and an I any non-initial word\n",
    "\n",
    "data = []\n",
    "for cor in corpus:                  \n",
    "    sent, _, tag = list(zip(*cor))  # sent, _, tag에 cor의 i번째 원소들을 묶어서 리턴 \n",
    "    data.append([sent, tag])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Maar', 'Conj', 'O'),\n",
       " ('we', 'Pron', 'O'),\n",
       " ('verwachtten', 'V', 'O'),\n",
       " ('eigenlijk', 'Adj', 'O'),\n",
       " ('iets', 'Pron', 'O'),\n",
       " (\"extra's\", 'N', 'O'),\n",
       " ('uit', 'Prep', 'O'),\n",
       " ('de', 'Art', 'O'),\n",
       " ('wonderoogst', 'N', 'O'),\n",
       " ('1997', 'Num', 'O'),\n",
       " ('.', 'Punc', 'O')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Maar',\n",
       " 'we',\n",
       " 'verwachtten',\n",
       " 'eigenlijk',\n",
       " 'iets',\n",
       " \"extra's\",\n",
       " 'uit',\n",
       " 'de',\n",
       " 'wonderoogst',\n",
       " '1997',\n",
       " '.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Conj', 'Pron', 'V', 'Adj', 'Pron', 'N', 'Prep', 'Art', 'N', 'Num', 'Punc')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 \n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35651\n",
      "[('Sao', 'Paulo', '(', 'Brasil', ')', ',', '23', 'may', '(', 'EFECOM', ')', '.'), ('B-LOC', 'I-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents, tags = list(zip(*data))    # data의 i번째 원소들을 묶어서 반환\n",
    "vocab = list(set(flatten(sents))) # set(): 중복 값 제거\n",
    "tagset = list(set(flatten(tags))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {'<UNK>': 0, '<DUMMY>': 1}  # dummy token is for start or end of sentence\n",
    "\n",
    "# word2index - word : index 형태 \n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "\n",
    "# index2word - index : word 형태로 변경\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "# tag2index - tag : index 형태\n",
    "tag2index = {}\n",
    "for tag in tagset:\n",
    "    if tag2index.get(tag) is None:\n",
    "        tag2index[tag] = len(tag2index)\n",
    "\n",
    "# index2tag - index : tag 형태로 변경 \n",
    "index2tag = {v:k for k, v in tag2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/04.window-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "windows = []\n",
    "\n",
    "for sample in data:  # data에는 sent, tag의 데이터가 있음 \n",
    "    dummy = ['<DUMMY>'] * WINDOW_SIZE\n",
    "    # 시작과 끝에 dummy넣어주고, (WINDOW_SIZE * 2 + 1)길이의 window를 생성\n",
    "    window = list(nltk.ngrams(dummy+list(sample[0])+dummy, WINDOW_SIZE*2+1))  \n",
    "    windows.extend([[list(window[i]), sample[1][i]] for i in range(len(sample[0]))])  # window에 tag추가해서 windows생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<DUMMY>', '<DUMMY>', 'Maar', 'we', 'verwachtten'),\n",
       " ('<DUMMY>', 'Maar', 'we', 'verwachtten', 'eigenlijk'),\n",
       " ('Maar', 'we', 'verwachtten', 'eigenlijk', 'iets'),\n",
       " ('we', 'verwachtten', 'eigenlijk', 'iets', \"extra's\"),\n",
       " ('verwachtten', 'eigenlijk', 'iets', \"extra's\", 'uit'),\n",
       " ('eigenlijk', 'iets', \"extra's\", 'uit', 'de'),\n",
       " ('iets', \"extra's\", 'uit', 'de', 'wonderoogst'),\n",
       " (\"extra's\", 'uit', 'de', 'wonderoogst', '1997'),\n",
       " ('uit', 'de', 'wonderoogst', '1997', '.'),\n",
       " ('de', 'wonderoogst', '1997', '.', '<DUMMY>'),\n",
       " ('wonderoogst', '1997', '.', '<DUMMY>', '<DUMMY>')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<DUMMY>', '<DUMMY>', 'Sao', 'Paulo', '('], 'B-LOC']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678377"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(windows)  # windows 목록들의 순서 섞이도록 \n",
    "\n",
    "train_data = windows[:int(len(windows) * 0.9)]  # 처음부터 int(len(windows)*0.9)의 index까지의 train_data 생성 \n",
    "test_data = windows[int(len(windows) * 0.9):]   # 인덱스 int(len(windows)*0.9)부터 끝까지 test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/04.window-classifier-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WindowClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, hidden_size, output_size):\n",
    "        super(WindowClassifier, self).__init__()  \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)  # nn.Embedding(embedding할 사이즈, 각 embedding vector의 사이즈)\n",
    "        self.h_layer1 = nn.Linear(embedding_size*(window_size*2+1), hidden_size)\n",
    "        self.h_layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.3)  # probability of an element to be zeroed = 0.3\n",
    "        \n",
    "    def forward(self, inputs, is_training=False): \n",
    "        embeds = self.embed(inputs)  # Batch x Window x D\n",
    "        concated = embeds.view(-1, embeds.size(1)*embeds.size(2))  # B x (W * D), row는 그대로 \n",
    "        h0 = self.relu(self.h_layer1(concated))\n",
    "        if is_training:\n",
    "            h0 = self.dropout(h0)\n",
    "        h1 = self.relu(self.h_layer2(h0))\n",
    "        if is_training:\n",
    "            h1 = self.dropout(h1)\n",
    "        out = self.softmax(self.o_layer(h1))\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 50\n",
    "HIDDEN_SIZE = 300\n",
    "EPOCH = 3\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# __init__(self, vocab_size, embedding_size, window_size, hidden_size, output_size)\n",
    "model = WindowClassifier(len(word2index), EMBEDDING_SIZE, WINDOW_SIZE, HIDDEN_SIZE, len(tag2index))\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], \n",
    "                    seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]]) if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))\n",
    "\n",
    "def prepare_tag(tag,tag2index):\n",
    "    return Variable(LongTensor([tag2index[tag]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] mean_loss : 2.10\n",
      "[1/3] mean_loss : 0.47\n",
      "[1/3] mean_loss : 0.37\n",
      "[1/3] mean_loss : 0.32\n",
      "[1/3] mean_loss : 0.28\n",
      "[2/3] mean_loss : 0.21\n",
      "[2/3] mean_loss : 0.22\n",
      "[2/3] mean_loss : 0.21\n",
      "[2/3] mean_loss : 0.20\n",
      "[2/3] mean_loss : 0.19\n",
      "[3/3] mean_loss : 0.12\n",
      "[3/3] mean_loss : 0.15\n",
      "[3/3] mean_loss : 0.14\n",
      "[3/3] mean_loss : 0.14\n",
      "[3/3] mean_loss : 0.14\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        x, y = list(zip(*batch))  # batch의 i번째 원소들끼리 묶어서 x와 y(=label)에 각각 반환\n",
    "        inputs = torch.cat([prepare_sequence(sent, word2index).view(1, -1) for sent in x]) # x의 개수만큼의 sent에 대해서 \n",
    "        targets = torch.cat([prepare_tag(tag, tag2index) for tag in y])                    # y의 개수만큼의 tag에 대해서 \n",
    "       \n",
    "        model.zero_grad()\n",
    "        preds = model(inputs, is_training=True)\n",
    "        loss = loss_function(preds, targets)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()  # update the parameters\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch+1, EPOCH, np.mean(losses)))\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.76491052212624\n"
     ]
    }
   ],
   "source": [
    "for_f1_score = [] \n",
    "\n",
    "accuracy = 0\n",
    "for test in test_data:\n",
    "    x, y = test[0], test[1]\n",
    "    input_ = prepare_sequence(x, word2index).view(1, -1)\n",
    "    \n",
    "    i = model(input_).max(1)[1] \n",
    "    pred = index2tag[i.data.tolist()[0]]  # model의 output으로 나온 index i를 가지고 그에 해당하는 tag를 알아낸다. \n",
    "    for_f1_score.append([pred, y])\n",
    "    if pred == y:\n",
    "        accuracy += 1\n",
    "    \n",
    "print(accuracy/len(test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'Madrid', ',', 'Juan', 'Carlos']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-19.1951, -20.8160,  -0.0001, -21.8554, -10.6259, -19.3503, -17.5546,\n",
       "          -9.9770, -21.2898]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_).max(1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This high score is because most of labels are '0' tag. \n",
    "So we need to measure f1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred, y_test = list(zip(*for_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '0' 제외 \n",
    "sorted_labels = sorted(\n",
    "    list(set(y_test) - {'O'}),\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC      0.799     0.686     0.738      1136\n",
      "      I-LOC      0.651     0.478     0.551       320\n",
      "     B-MISC      0.705     0.429     0.534       801\n",
      "     I-MISC      0.640     0.358     0.459       646\n",
      "      B-ORG      0.754     0.709     0.731      1343\n",
      "      I-ORG      0.723     0.730     0.726       917\n",
      "      B-PER      0.807     0.762     0.784      1304\n",
      "      I-PER      0.894     0.806     0.848       961\n",
      "\n",
      "avg / total      0.765     0.659     0.703      7428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
